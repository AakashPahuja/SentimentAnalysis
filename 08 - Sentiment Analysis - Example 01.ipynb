{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "In this lab we will apply the Naive Bayes classifier for the sentiment analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "import math\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining all the function to read data, extract features and traing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the datasets\n",
    "path = './data/'\n",
    "filePrefix = 'training_'\n",
    "categories=['POS','NEG']\n",
    "dataset={}\n",
    "dataset_raw = {}\n",
    "allFeatures=set()\n",
    "tot_articles = 0\n",
    "articles_count={}\n",
    "\n",
    "N={} # Number of posts in each corpus\n",
    "\n",
    "for category in categories:\n",
    "    fileName=path+filePrefix+category.lower()\n",
    "    f=open(fileName,'r')\n",
    "    text = ''\n",
    "    text_raw = ''    \n",
    "    lines=f.readlines()\n",
    "    tot_articles+=len(lines)\n",
    "    articles_count[category] = len(lines)\n",
    "    dataset_raw[category] = list(map(lambda line: line.lower(), lines))\n",
    "    \n",
    "    for line in lines:\n",
    "        text+=line.replace('\\n',' ').lower()\n",
    "        text_raw = line.lower()\n",
    "    f.close\n",
    "    N[category]=len(lines)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    dataset[category] = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990\n",
      "991\n",
      "1981\n"
     ]
    }
   ],
   "source": [
    "print (N['POS'])\n",
    "print (N['NEG'])\n",
    "print (tot_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying basic pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing POS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "dataset_clean={}\n",
    "\n",
    "def apply_stopwording(corpus, min_len):\n",
    "    punctuations = \".,\\\"-\\\\/#!?$%\\^&\\*;:{}=\\-_'~()\"    \n",
    "    filtered_corpus = [token for token in corpus if (not token in stopwords.words('english') and len(token)>min_len) and (not token in punctuations)]\n",
    "    return filtered_corpus\n",
    "\n",
    "#Let's remove punctuation characters and apply stopwording\n",
    "for category in categories:\n",
    "    print ('Processing %s' % category)\n",
    "    dataset_clean[category] = apply_stopwording(dataset[category],3)\n",
    "    print (dataset_clean[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing POS\n",
      "Processing NEG\n"
     ]
    }
   ],
   "source": [
    "dataset_final={}\n",
    "all_words=set()\n",
    "\n",
    "for category in categories:\n",
    "    print ('Processing %s' % category)\n",
    "    dataset_final[category]=dataset_clean[category]\n",
    "    for token in dataset_final[category]:\n",
    "        all_words.add(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the probabilities for each category (you can define any number of categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS - p(POS)=0.49974760222110043\n",
      "NEG - p(NEG)=0.5002523977788995\n"
     ]
    }
   ],
   "source": [
    "feature_count = {}\n",
    "category_count = {}\n",
    "probCat = {}\n",
    "\n",
    "# Calculate the probabilities for each category\n",
    "for category in categories:\n",
    "    probCat[category]=articles_count[category]*1.0/tot_articles\n",
    "    print (\"%s - p(%s)=%s\" % (category,category,probCat[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating term probabilities ð‘(ð‘¡|C) and ð‘(ð‘¡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqWord = {}\n",
    "wordCounts = {}\n",
    "\n",
    "def buildFrequencies(data):\n",
    "    for category in categories:\n",
    "        freqWord[category] = FreqDist(data[category])\n",
    "        wordCounts[category] = len(data[category])\n",
    "\n",
    "\n",
    "#Generate frequencies: the implementation is very different from the first version\n",
    "buildFrequencies(dataset_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Frequencies for word 'stiller':\n",
      "F('stiller'|'POS')=43\n",
      "F('stiller'|'NEG')=17\n"
     ]
    }
   ],
   "source": [
    "word = 'stiller'\n",
    "print (\"Checking Frequencies for word '%s':\" % word)\n",
    "print (\"F('%s'|'POS')=%s\" % (word,freqWord['POS'][word]))\n",
    "print (\"F('%s'|'NEG')=%s\" % (word,freqWord['NEG'][word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From frequencies to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTermProbability(word):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for category in categories:\n",
    "        total += wordCounts[category]\n",
    "        if word in freqWord[category]:\n",
    "            count+=freqWord[category][word]\n",
    "    return count*1.0/total\n",
    "\n",
    "def getTermCondProbability(word,category):\n",
    "    count = 0\n",
    "    total = wordCounts[category]\n",
    "\n",
    "    if word in freqWord[category]:\n",
    "        count=freqWord[category][word]\n",
    "    else:\n",
    "        #Apply Laplace Smoothing\n",
    "        count=1\n",
    "    \n",
    "    return count*1.0/(total+len(all_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability for word 'good' - p('good')=0.00046255813658771495\n",
      "probability for word 'good' in POS - p('good'|'POS')=0.0030925825435042594\n",
      "probability for word 'good' in NEG - p('good'|'NEG')=0.0032756721280735065\n"
     ]
    }
   ],
   "source": [
    "word = 'good'\n",
    "print (\"probability for word '%s' - p('%s')=%s\" % (word,word,getTermProbability('team')))\n",
    "print (\"probability for word '%s' in POS - p('%s'|'POS')=%s\" % (word,word,getTermCondProbability(word,'POS')))\n",
    "print (\"probability for word '%s' in NEG - p('%s'|'NEG')=%s\" % (word,word,getTermCondProbability(word,'NEG')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Naive Bayes Classifier\n",
    "\n",
    "Here we use the logs of probabilities instead of classic definition of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NaiveBayesClassifier(article):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(article)\n",
    "    text = nltk.Text(tokens)\n",
    "    clean_text = apply_stopwording(remove_punctuation(text), 3)\n",
    "    words = [w for w in clean_text]\n",
    "    results={}\n",
    "    for category in categories:\n",
    "        pCat = probCat[category]\n",
    "        pNumerator = 1.0\n",
    "        idx = 1\n",
    "        for word in words:\n",
    "            pN = getTermCondProbability(word,category)\n",
    "            #pNumerator = pNumerator * pN\n",
    "            pNumerator+= math.log(pN)\n",
    "            #print '[%s] p(%s)=%s' % (category, word,pN)\n",
    "            #idx+=1\n",
    "            #print '[%s][%s - %s] %s' % (idx,category, word, pNumerator)\n",
    "\n",
    "        pClassification = pNumerator+math.log(pCat)\n",
    "        results[category] = pClassification\n",
    "        #print '[%s] p()=%s' % (category, pClassification)\n",
    "    \n",
    "    pMax = -10000000\n",
    "    predictedClass = ''\n",
    "    for category in categories:\n",
    "        if results[category]>pMax:\n",
    "            pMax = results[category]\n",
    "            predictedClass = category\n",
    "\n",
    "    #print ('The article has been assigned to class \"%s\" with a probability of %s' % (predictedClass,pMax))\n",
    "    return predictedClass,pMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NEG', -47.44494284739373)\n"
     ]
    }
   ],
   "source": [
    "article = \"I think this business proposition is risky to say the least.\"\n",
    "article = \"I think this business proposition makes perfect sense.\"\n",
    "print (NaiveBayesClassifier(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Prediction[POS] Class[POS]\n",
      "2. Prediction[POS] Class[POS]\n",
      "3. Prediction[POS] Class[POS]\n",
      "4. Prediction[POS] Class[POS]\n",
      "5. Prediction[POS] Class[POS]\n",
      "6. Prediction[POS] Class[POS]\n",
      "7. Prediction[POS] Class[POS]\n",
      "8. Prediction[POS] Class[POS]\n",
      "9. Prediction[POS] Class[POS]\n",
      "10. Prediction[POS] Class[POS]\n",
      "11. Prediction[NEG] Class[NEG]\n",
      "12. Prediction[NEG] Class[NEG]\n",
      "13. Prediction[NEG] Class[NEG]\n",
      "14. Prediction[NEG] Class[NEG]\n",
      "15. Prediction[NEG] Class[NEG]\n",
      "16. Prediction[NEG] Class[NEG]\n",
      "17. Prediction[NEG] Class[NEG]\n",
      "18. Prediction[NEG] Class[NEG]\n",
      "19. Prediction[NEG] Class[NEG]\n",
      "20. Prediction[NEG] Class[NEG]\n",
      "\n",
      "The classifer was correct 20 out of 20 or 1.0\n",
      "precision=1.0\n",
      "recall=1.0\n",
      "F=1.0\n"
     ]
    }
   ],
   "source": [
    "def get_string_from_corpus(corpus):\n",
    "    result=''\n",
    "    for token in corpus:\n",
    "        result+=token\n",
    "        result+=' '\n",
    "    return result[:-1]\n",
    "\n",
    "f=open('./data/testing_sentiment.txt','r')\n",
    "lines=f.readlines()\n",
    "f.close\n",
    "\n",
    "correct = 0\n",
    "total = len(lines)\n",
    "index = 1\n",
    "\n",
    "TP=0.0\n",
    "TN=0.0\n",
    "FP=0.0\n",
    "FN=0.0\n",
    "\n",
    "F=0.0\n",
    "precision = 0.0\n",
    "recall = 0.0\n",
    "\n",
    "test_articles = {}\n",
    "\n",
    "for line in lines:\n",
    "    elems = line.split('\\t')\n",
    "    article=elems[0]\n",
    "    category=elems[1][:-1]\n",
    "\n",
    "    #Clean up the article and apply normalization\n",
    "    text = nltk.Text(nltk.word_tokenize(article.lower()))\n",
    "    clean_article = get_string_from_corpus(apply_stopwording(remove_punctuation(text), 3))\n",
    "    test_articles[clean_article]=category\n",
    "    \n",
    "    predictedCategory,pCategory = NaiveBayesClassifier(str(clean_article))\n",
    "    \n",
    "    print ('%s. Prediction[%s] Class[%s]' % (index,predictedCategory,category))\n",
    "    index+=1\n",
    "    \n",
    "    #Calculating quality measures\n",
    "    if (predictedCategory == category):\n",
    "        correct+=1\n",
    "        if (category == categories[1]):\n",
    "            TP+=1\n",
    "        else:\n",
    "            TN+=1\n",
    "    else:\n",
    "        if (predictedCategory == categories[1]):\n",
    "            FN+=1\n",
    "        else:\n",
    "            FP+=1\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F=2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print ('\\nThe classifer was correct %s out of %s or %s' % (correct,total,correct*1.0/total))\n",
    "print ('precision=%s' % precision)\n",
    "print ('recall=%s' % recall)\n",
    "print ('F=%s' % F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Can we use this trained Naive Bayes Classifier for our Tweets?\n",
    "The idea here is to use a model trained on a different dataset and use it for Tweeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225980\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def remove_utf(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "\n",
    "def remove_tinyURL(text):\n",
    "    return re.sub(r'http\\S+',r'',text)\n",
    "\n",
    "path = \".\\\\data\\\\Tweet-3000.txt\"\n",
    "\n",
    "tweets = \"\"\n",
    "file_input = open (path,\"r\")\n",
    "lines = file_input.readlines()\n",
    "for line in lines:\n",
    "    tweets += (remove_utf(line.lower()))\n",
    "file_input.close()\n",
    "\n",
    "print (len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "tokens_raw = tweet_tokenizer.tokenize(tweets.lower())\n",
    "tokens = []\n",
    "for token in tokens_raw:\n",
    "    if (token == \" \"):\n",
    "        continue\n",
    "    elif (token.startswith('http')):\n",
    "        tokens.append('URL')\n",
    "    elif (token.startswith('@')):\n",
    "        tokens.append('USER_NAME')\n",
    "    else:\n",
    "        tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lemma(token):\n",
    "    #Return the morphological variant of this word\n",
    "    lemma = wordnet.morphy(token)\n",
    "\n",
    "    if lemma is None:\n",
    "        return token\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "lemmas = [get_lemma(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words= set(nltk.corpus.stopwords.words('english'))\n",
    "tokens_clean = [token for token in lemmas if (len(token)>4 and token not in stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweets(text):\n",
    "    tokens_raw = tweet_tokenizer.tokenize(text)\n",
    "    tokens = []\n",
    "    for token in tokens_raw:\n",
    "        if (token == \" \" or token.startswith('http') or token.startswith('@')):\n",
    "            continue\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "    \n",
    "    lemmas = [get_lemma(token) for token in tokens]\n",
    "    tokens_clean = [token for token in lemmas if (len(token)>4 and token not in stop_words)]\n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "file_input = open (path,\"r\")\n",
    "lines = file_input.readlines()\n",
    "for line in lines:\n",
    "    dataset.append(remove_utf(line.lower()))\n",
    "file_input.close()\n",
    "\n",
    "tweets = [clean_tweets(tweet) for tweet in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Tweeter: 2982\n",
      "Positive Tweeter: 18\n"
     ]
    }
   ],
   "source": [
    "totNeg = 0\n",
    "totPos = 0\n",
    "for tweet in tweets:\n",
    "    #print (tweet)\n",
    "    predictedCategory,pCategory = NaiveBayesClassifier(str(tweet))\n",
    "    if (predictedCategory == \"POS\"):\n",
    "        totPos = totPos + 1\n",
    "        print (\"POS -\"+ tweet)\n",
    "    else:\n",
    "        totNeg = totNeg + 1\n",
    "        r = random.randint(1,101)\n",
    "        if r<=2:\n",
    "            print (\"NEG - %s\" % tweet)\n",
    "        \n",
    "print (\"Negative Tweeter: %s\" % totNeg)\n",
    "print (\"Positive Tweeter: %s\" % totPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
